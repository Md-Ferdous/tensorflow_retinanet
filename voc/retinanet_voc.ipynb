{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install tensorflow-datasets\nimport cv2\nimport json\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport os\n\ntf.enable_v2_behavior()\nprint('Tensorflow', tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imshow(image):\n    plt.figure(figsize=(12, 12))\n    plt.axis('off')\n    plt.imshow(image)\n\n\ndef compute_anchor_dimensions(ratios=[0.5, 1, 2],\n                              scales=[1, 1.25, 1.58],\n                              areas=[32 * 32, 64 * 64, 128 * 128, 256 * 256, 512 * 512]):\n    anchor_shapes = {'P{}'.format(i): [] for i in range(3, 8)}\n    for area in areas:\n        for ratio in ratios:\n            a_h = np.sqrt(area / ratio)\n            a_w = area / a_h\n            for scale in scales:\n                h = np.int32(scale * a_h)\n                w = np.int32(scale * a_w)\n                anchor_shapes['P{}'.format(\n                    int(np.log2(np.sqrt(area) // 4)))].append([w, h])\n        anchor_shapes['P{}'.format(int(np.log2(np.sqrt(area) // 4)))] = np.array(\n            anchor_shapes['P{}'.format(int(np.log2(np.sqrt(area) // 4)))])\n    return anchor_shapes\n\n\ndef get_anchors(input_shape=512, tensor=True):\n    anchor_dimensions = compute_anchor_dimensions()\n    anchors = []\n    for i in range(3, 8):\n        feature_name = 'P{}'.format(i)\n        stride = 2**i\n        feature_size = (input_shape) // stride\n\n        dims = anchor_dimensions[feature_name]\n        dims = dims[None, None, ...]\n        dims = np.tile(dims, reps=[feature_size, feature_size, 1, 1])\n\n        rx = (np.arange(feature_size) + 0.5) * (stride)\n        ry = (np.arange(feature_size) + 0.5) * (stride)\n        sx, sy = np.meshgrid(rx, ry)\n        cxy = np.stack([sx, sy], axis=-1)\n        cxy = cxy[:, :, None, :]\n        cxy = np.tile(cxy, reps=[1, 1, 9, 1])\n        anchors.append(np.reshape(\n            np.concatenate([cxy, dims], axis=-1), [-1, 4]))\n    anchors = np.concatenate(anchors, axis=0)\n    if tensor:\n        anchors = tf.constant(anchors, dtype=tf.float32)\n    return anchors\n\n\n@tf.function()\ndef compute_iou(boxes1, boxes2):\n    boxes1 = tf.cast(boxes1, dtype=tf.float32)\n    boxes2 = tf.cast(boxes2, dtype=tf.float32)\n\n    boxes1_t = change_box_format(boxes1, return_format='x1y1x2y2')\n    boxes2_t = change_box_format(boxes2, return_format='x1y1x2y2')\n\n    lu = tf.maximum(boxes1_t[:, None, :2], boxes2_t[:, :2])  # ld ru ??\n    rd = tf.minimum(boxes1_t[:, None, 2:], boxes2_t[:, 2:])\n\n    intersection = tf.maximum(0.0, rd - lu)\n    inter_square = intersection[:, :, 0] * intersection[:, :, 1]\n\n    square1 = boxes1[:, 2] * boxes1[:, 3]\n    square2 = boxes2[:, 2] * boxes2[:, 3]\n\n    union_square = tf.maximum(square1[:, None] + square2 - inter_square, 1e-10)\n    return tf.clip_by_value(inter_square / union_square, 0.0, 1.0)\n\n\ndef change_box_format(boxes, return_format='xywh'):\n    boxes = tf.cast(boxes, dtype=tf.float32)\n    if return_format == 'xywh':\n        # x1 y1 x2 y2\n        # 0  1  2  3\n        return tf.stack([(boxes[..., 2] + boxes[..., 0]) / 2.0,\n                         (boxes[..., 3] + boxes[..., 1]) / 2.0,\n                         boxes[..., 2] - boxes[..., 0],\n                         boxes[..., 3] - boxes[..., 1]], axis=-1)\n    elif return_format == 'x1y1x2y2':\n        # x  y  w  h\n        # 0  1  2  3\n        return tf.stack([boxes[..., 0] - boxes[..., 2] / 2.0,\n                         boxes[..., 1] - boxes[..., 3] / 2.0,\n                         boxes[..., 0] + boxes[..., 2] / 2.0,\n                         boxes[..., 1] + boxes[..., 3] / 2.0], axis=-1)\n    return 'You should not be here'\n\n\ndef draw_bboxes(image, bbox_list):\n    image = image / 255.\n    h, w = image.shape.as_list()[:2]\n    bboxes = tf.cast(tf.stack([\n        bbox_list[:, 1] / h, bbox_list[:, 0] /\n        w, bbox_list[:, 3] / h, bbox_list[:, 2] / w\n    ], axis=-1), dtype=tf.float32)\n    # To do, set colors for each class\n    colors = tf.random.uniform(maxval=1, shape=[bbox_list.shape[0], 3])\n    return tf.image.convert_image_dtype(tf.image.draw_bounding_boxes(image[None, ...],\n                                                                     bboxes[None, ...],\n                                                                     colors)[0, ...], dtype=tf.uint8)\n\ndef draw_boxes_cv2(image, bbox_list):\n    img = np.uint8(image).copy()\n    bbox_list = np.array(bbox_list, dtype=np.int32)\n    for box in bbox_list:\n        img = cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), [0, 0, 200], 3)\n    return img\n\n\ndef get_label(label_path, class_map, input_shape=512):\n    with open(label_path, 'r') as f:\n        temp = json.load(f)\n    bbox = []\n    class_ids = []\n    for obj in temp['frames'][0]['objects']:\n        if 'box2d' in obj:\n            x1 = obj['box2d']['x1']\n            y1 = obj['box2d']['y1']\n            x2 = obj['box2d']['x2']\n            y2 = obj['box2d']['y2']\n            bbox.append(np.array([x1, y1, x2, y2]))\n            class_ids.append(class_map[obj['category']])\n    bbox = np.array(bbox, dtype=np.float32)\n    H, W = 720, 1280.  # ToDO remove hardcoded values\n    bbox[:, 0] = bbox[:, 0] / W\n    bbox[:, 2] = bbox[:, 2] / W\n    bbox[:, 1] = bbox[:, 1] / H\n    bbox[:, 3] = bbox[:, 3] / H\n    bbox = np.int32(bbox * input_shape)\n    class_ids = np.array(class_ids, dtype=np.float32)[..., None]\n    return np.concatenate([bbox, class_ids], axis=-1)\n\n\n@tf.function\ndef get_image(image_path, input_shape=None):\n    H = W = input_shape\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img)\n    img = tf.image.resize(img, size=[H, W])\n    img = img[:, :, ::-1] - tf.constant([103.939, 116.779, 123.68])\n    return img\n\n\ndef load_data(input_shape=None):\n    def load_data(image_path, label):\n        images = get_image(image_path, input_shape=input_shape)\n        targets = encode_targets(label, input_shape=input_shape)\n        # To-do : transform bbox to account image resizing, add random_flip\n        return images, targets\n    return load_data\n\n\n@tf.function\ndef encode_targets(label, input_shape=None):\n    \"\"\"We use the assignment rule from RPN.\n        Faster RCNN box coder follows the coding schema described below:\n            ty = (y - ya) / ha\n            tx = (x - xa) / wa\n            th = log(h / ha)\n            tw = log(w / wa)\n        where x, y, w, h denote the box's center coordinates, width and height\n        respectively. Similarly, xa, ya, wa, ha denote the anchor's center\n        coordinates, width and height. tx, ty, tw and th denote the\n        anchor-encoded center, width and height respectively.\n        The open-source implementation recommends using [10.0, 10.0, 5.0, 5.0] as\n        scale factors.\n        See http://arxiv.org/abs/1506.01497 for details. \n        Set achors with iou < 0.5 to 0 and\n        set achors with iou iou > 0.4 && < 0.5 to -1. Convert\n        regression targets into one-hot encoding (N, #anchors, n_classes + 1)\n        in loss_fn and exclude background class in loss calculation.\n        Use [0, 0, 0, ... 0, n_classes] (all units set to zeros) to represent\n        background class.\n    \"\"\"\n    scale_factors = tf.constant([5.0, 5.0, 5.0, 5.0])\n    anchors = get_anchors(input_shape=input_shape, tensor=True)\n    gt_boxes = label[:, :4]\n    gt_boxes = change_box_format(gt_boxes, return_format='xywh')\n    gt_class_ids = label[:, 4]\n    ious = compute_iou(anchors, gt_boxes)\n\n    max_ious = tf.reduce_max(ious, axis=1)\n    max_ids = tf.argmax(ious, axis=1, output_type=tf.int32)\n\n    background_mask = max_ious > 0.5\n    ignore_mask = tf.logical_and(max_ious > 0.4, max_ious < 0.5)\n\n\n    selected_gt_boxes = tf.gather(gt_boxes, max_ids)\n    selected_gt_class_ids = 1. + tf.gather(gt_class_ids, max_ids)\n\n    selected_gt_class_ids = selected_gt_class_ids * \\\n        tf.cast(background_mask, dtype=tf.float32)\n    classification_targets = selected_gt_class_ids - \\\n        tf.cast(\n            ignore_mask, dtype=tf.float32)\n    regression_targets = tf.stack([\n        (selected_gt_boxes[:, 0] - anchors[:, 0]) / anchors[:, 2],\n        (selected_gt_boxes[:, 1] - anchors[:, 1]) / anchors[:, 3],\n        tf.math.log(selected_gt_boxes[:, 2] / anchors[:, 2]),\n        tf.math.log(selected_gt_boxes[:, 3] / anchors[:, 3])\n    ], axis=-1)\n    regression_targets = regression_targets * scale_factors\n    return (tf.cast(classification_targets, dtype=tf.int32),\n            regression_targets,\n            background_mask,\n            ignore_mask)\n\n\ndef decode_targets(classification_outputs,\n                   regression_outputs,\n                   input_shape=512,\n                   classification_threshold=0.05,\n                   nms_threshold=0.5):\n    scale_factors = tf.constant([5.0, 5.0, 5.0, 5.0])\n    anchors = get_anchors(input_shape=input_shape, tensor=True)\n\n    '''gt targets are in one hot form, no need to apply  sigmoid to check correctness, use sigmoid during actual inference'''\n    class_ids = tf.argmax(classification_outputs, axis=-1)\n    # confidence_scores = tf.reduce_max(classification_outputs, axis=-1)\n    confidence_scores = tf.reduce_max(  \n        tf.nn.sigmoid(classification_outputs), axis=-1)   \n    regression_outputs = regression_outputs / scale_factors\n    boxes = tf.concat([(regression_outputs[:, :2] * anchors[:, 2:] + anchors[:, :2]),\n                       tf.math.exp(regression_outputs[:, 2:]) * anchors[:, 2:]\n                       ], axis=-1)\n    boxes = change_box_format(boxes, return_format='x1y1x2y2')\n                          \n    nms_indices = tf.image.non_max_suppression(boxes,\n                                               confidence_scores,\n                                               score_threshold=classification_threshold,\n                                               iou_threshold=nms_threshold,\n                                               max_output_size=200)\n    final_class_ids = tf.gather(class_ids, nms_indices)\n    final_scores = tf.gather(confidence_scores, nms_indices)\n    final_boxes = tf.cast(tf.gather(boxes, nms_indices), dtype=tf.int32)\n\n    matched_anchors = tf.gather(anchors, tf.where(confidence_scores > classification_threshold)[:, 0])\n    matched_anchors = tf.cast(change_box_format(matched_anchors, return_format='x1y1x2y2'),\n                          dtype=tf.int32)    \n    return final_boxes, final_class_ids, final_scores, matched_anchors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv_block(x,\n               n_filters,\n               size,\n               strides=1,\n               kernel_init='he_normal',\n               bias_init='zeros',\n               bn_activated=False, name=''):\n    x = tf.keras.layers.Conv2D(filters=n_filters,\n                               kernel_size=size,\n                               padding='same',\n                               strides=strides,\n                               kernel_initializer=kernel_init,\n                               bias_initializer=bias_init,\n                               name='conv_' + name if name else None)(x)\n    if bn_activated:\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.ReLU()(x)\n    return x\n\n\ndef Upsampling(tensor, scale=2):\n    dims = tensor.shape.as_list()[1:-1]\n    return tf.image.resize(tensor, size=[dims[0] * scale, dims[1] * scale])\n\n\ndef build_classification_subnet(n_classes=None, n_anchors=9, p=0.01):\n    input_layer = tf.keras.layers.Input(shape=[None, None, 256])\n    x = input_layer\n    for i in range(4):\n        x = conv_block(\n            x, 256, 3, kernel_init=tf.keras.initializers.RandomNormal(0.0, 0.01))\n        x = tf.keras.layers.ReLU()(x)\n    bias_init = -np.log((1 - p) / p)\n    output_layer = tf.keras.layers.Conv2D(filters=n_classes * n_anchors,\n                                          kernel_size=3,\n                                          padding='same',\n                                          kernel_initializer=tf.keras.initializers.RandomNormal(\n                                              0.0, 0.01),\n                                          bias_initializer=tf.keras.initializers.Constant(\n                                              value=bias_init),\n                                          activation=None)(x)\n    output_layer = tf.keras.layers.Reshape(\n        target_shape=[-1, n_classes])(output_layer)\n    return tf.keras.Model(inputs=input_layer, outputs=output_layer, name='classification_subnet')\n\n\ndef build_regression_subnet(n_anchors=9):\n    input_layer = tf.keras.layers.Input(shape=[None, None, 256])\n    x = input_layer\n    for i in range(4):\n        x = conv_block(\n            x, 256, 3, kernel_init=tf.keras.initializers.RandomNormal(0.0, 0.01))\n        x = tf.keras.layers.ReLU()(x)\n    output_layer = tf.keras.layers.Conv2D(filters=4 * n_anchors,\n                                          kernel_size=3,\n                                          padding='same',\n                                          kernel_initializer=tf.keras.initializers.RandomNormal(\n                                              0.0, 0.01),\n                                          bias_initializer=tf.keras.initializers.zeros(),\n                                          activation=None)(x)\n    output_layer = tf.keras.layers.Reshape(target_shape=[-1, 4])(output_layer)\n    return tf.keras.Model(inputs=input_layer, outputs=output_layer, name='regression_subnet')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LossV2():\n    def __init__(self, batch_size=None, n_classes=None):\n        self.smooth_l1 = tf.losses.Huber(reduction='none')\n        self.num_classes = n_classes\n        self.global_batch_size = batch_size\n\n    def focal_loss(self, y_true, y_pred, alpha=0.25, gamma=2):\n        y_true = tf.one_hot(y_true, depth=self.num_classes + 1)\n        y_true = y_true[:, :, 1:]\n        y_pred = tf.sigmoid(y_pred)\n\n        at = alpha * y_true + (1 - y_true) * (1 - alpha)\n        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n        f_loss = -at * tf.pow(1 - pt, gamma) * tf.math.log(pt)\n        return f_loss\n\n    def __call__(self,\n                 classification_targets,\n                 classification_predictions,\n                 regression_targets,\n                 regression_predictions,\n                 background_mask,\n                 ignore_mask):\n        num_positive_detections = tf.maximum(tf.reduce_sum(\n            tf.cast(background_mask, dtype=tf.float32), axis=-1), 1.0)\n\n        positive_classification_mask = tf.expand_dims(\n            tf.logical_not(ignore_mask), axis=-1)\n        positive_classification_mask = tf.tile(\n            positive_classification_mask, multiples=[1, 1, self.num_classes])\n\n        positive_regression_mask = tf.expand_dims(background_mask, axis=-1)\n        positive_regression_mask = tf.tile(\n            positive_regression_mask, multiples=[1, 1, 4])\n\n        Lcls = self.focal_loss(classification_targets,\n                               classification_predictions)\n        Lreg = self.smooth_l1(regression_targets, regression_predictions)\n        Lcls = Lcls * tf.cast(positive_classification_mask, dtype=tf.float32)\n        Lreg = Lreg * tf.cast(positive_regression_mask, dtype=tf.float32)\n\n        Lcls = tf.reduce_sum(\n            Lcls, axis=[1, 2]) / num_positive_detections\n        Lreg = tf.reduce_sum(\n            Lreg, axis=[1, 2]) / num_positive_detections\n        Lcls = tf.reduce_sum(Lcls) * 1./ self.global_batch_size\n        Lreg = tf.reduce_sum(Lreg) * 1./ self.global_batch_size\n        return Lreg, Lcls, tf.reduce_mean(num_positive_detections)\n\nclass Loss():\n    def __init__(self, n_classes=None):\n        self.smooth_l1 = tf.losses.Huber(delta=0.1, reduction='none')\n        self.num_classes = n_classes\n\n    def focal_loss(self, y_true, y_pred, alpha=0.25, gamma=2):\n        y_true = tf.one_hot(y_true, depth=self.num_classes + 1)\n        y_true = y_true[:, 1:]\n        y_pred = tf.sigmoid(y_pred)\n\n        at = alpha * y_true + (1 - y_true) * (1 - alpha)\n        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n        f_loss = -at * tf.pow(1 - pt, gamma) * tf.math.log(pt)\n        return f_loss\n\n    def __call__(self,\n                 classification_targets,\n                 classification_predictions,\n                 regression_targets,\n                 regression_predictions,\n                 background_mask,\n                 ignore_mask):\n        num_positive_detections = tf.maximum(tf.reduce_sum(\n            tf.cast(background_mask, dtype=tf.float32)), 1.0)\n        positive_classification_mask = tf.logical_not(ignore_mask)\n\n        regression_targets_positive = tf.boolean_mask(\n            regression_targets, background_mask)\n        regression_predictions_positive = tf.boolean_mask(\n            regression_predictions, background_mask)\n\n        classification_targets_positive = tf.boolean_mask(\n            classification_targets, positive_classification_mask)\n        classification_predictions_positive = tf.boolean_mask(\n            classification_predictions, positive_classification_mask)\n\n        Lreg = tf.reduce_sum(self.smooth_l1(regression_targets_positive,\n                                            regression_predictions_positive, delta=0.1))\n        Lcls = tf.reduce_sum(self.focal_loss(classification_targets_positive,\n                                             classification_predictions_positive))\n        return (Lreg + Lcls) / num_positive_detections\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RetinaNet(input_shape=None, n_classes=None):\n    H = W = input_shape\n    base_model = tf.keras.applications.ResNet50(\n        input_shape=[H, W, 3], weights='imagenet', include_top=False)\n\n    resnet_block_output_names = ['activation_21', 'activation_39', 'activation_48']\n\n    resnet_block_outputs = {'C{}'.format(idx + 3): base_model.get_layer(\n        layer).output for idx, layer in enumerate(resnet_block_output_names)}\n    resnet_block_outputs = {level: conv_block(\n        tensor, 256, 1, name=level + '_1x1') for level, tensor in resnet_block_outputs.items()}\n\n    P5 = resnet_block_outputs['C5']\n    P6 = conv_block(base_model.get_layer(\n        'activation_48').output, 256, 3, strides=2, name='P6')\n    P6_relu = tf.keras.layers.ReLU(name='P6')(P6)\n    P7 = conv_block(P6_relu, 256, 3, strides=2, name='P7')\n    M4 = tf.keras.layers.add([tf.keras.layers.Lambda(Upsampling, arguments={'scale': 2}, name='P5_UP')(\n        P5), resnet_block_outputs['C4']], name='P4_merge')\n    M3 = tf.keras.layers.add([tf.keras.layers.Lambda(Upsampling, arguments={'scale': 2}, name='P4_UP')(\n        M4), resnet_block_outputs['C3']], name='P3_merge')\n    P4 = conv_block(M4, 256, 3, name='P4')\n    P3 = conv_block(M3, 256, 3, name='P3')\n    pyrammid_features = [P7, P6, P5, P4, P3]\n\n    classification_subnet = build_classification_subnet(n_classes=n_classes)\n    regression_subnet = build_regression_subnet()\n\n    classification_outputs = [classification_subnet(\n        level) for level in pyrammid_features]\n    regression_outputs = [regression_subnet(\n        level) for level in pyrammid_features]\n\n    classification_head = tf.keras.layers.concatenate(\n        classification_outputs, axis=1, name='classification_head')\n    regression_head = tf.keras.layers.concatenate(\n        regression_outputs, axis=1, name='regression_head')\n\n    return tf.keras.Model(inputs=base_model.input, outputs=[\n        classification_head, regression_head],\n        name='RetinaNet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_SHAPE = 640\nBATCH_SIZE = 4\nN_CLASSES = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef flip_data(image, boxes, w):\n  if tf.random.uniform(()) > 0.5:\n      image = tf.image.flip_left_right(image)\n      boxes = tf.stack([\n          w - boxes[:, 2],\n          boxes[:, 1],\n          w - boxes[:, 0],\n          boxes[:, 3]\n      ], axis=-1)\n  return image, boxes\n\ndef load_data(input_shape):\n  h, w = input_shape, input_shape\n  @tf.function\n  def load_data_(example, input_shape=input_shape):\n      image = tf.cast(example['image'], dtype=tf.float32)\n      boxes_ = example['objects']['bbox']\n      class_ids = tf.expand_dims(tf.cast(example['objects']['label'], dtype=tf.float32), axis=-1)\n      image = tf.image.resize(image, size=[h, w])\n      # convert y1x1y2x2 -> x1y1x2y2\n      boxes = tf.stack([\n          tf.clip_by_value(boxes_[:, 1] * w, 0, w),\n          tf.clip_by_value(boxes_[:, 0] * h, 0, h),\n          tf.clip_by_value(boxes_[:, 3] * w, 0, w),\n          tf.clip_by_value(boxes_[:, 2] * h, 0, h)\n      ], axis=-1)\n      image, boxes = flip_data(image, boxes, w)\n      label = tf.concat([boxes, class_ids], axis=-1)\n      cls_targets, reg_targets, bg, ig  = encode_targets(label, input_shape=input_shape)\n      return image, (cls_targets, reg_targets, bg, ig)\n  return load_data_\n\ntrain_dataset = tfds.load('voc2007', shuffle_files=False, split=['train'])[0]\ntrain_dataset = train_dataset.map(load_data(input_shape=INPUT_SHAPE), num_parallel_calls=tf.data.experimental.AUTOTUNE)\ntrain_dataset = train_dataset.batch(BATCH_SIZE)\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\nval_dataset = tfds.load('voc2007', shuffle_files=False, split=['validation'])[0]\nval_dataset = val_dataset.map(load_data(input_shape=INPUT_SHAPE), num_parallel_calls=tf.data.experimental.AUTOTUNE)\nval_dataset = val_dataset.batch(BATCH_SIZE)\nval_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\ntrain_dataset, val_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nfor batch in train_dataset.take(1):\n    break\nimage = batch[0][i]\ncls_targets, reg_targets, _, _ = batch[1]\nclassification_outputs = tf.one_hot(cls_targets[i], depth=N_CLASSES + 1)[:, 1:]\nregression_outputs = reg_targets[i]\n\nscale_factors = tf.constant([5.0, 5.0, 5.0, 5.0])\nanchors = get_anchors(input_shape=INPUT_SHAPE, tensor=True)\nclass_ids = tf.argmax(classification_outputs, axis=-1)\nconfidence_scores = tf.reduce_max(classification_outputs, axis=-1)\nregression_outputs = regression_outputs / scale_factors\nboxes = tf.concat([(regression_outputs[:, :2] * anchors[:, 2:] + anchors[:, :2]),\n                   tf.math.exp(regression_outputs[:, 2:]) * anchors[:, 2:]\n                   ], axis=-1)\nboxes = change_box_format(boxes, return_format='x1y1x2y2')\n\nnms_indices = tf.image.non_max_suppression(boxes,\n                                           confidence_scores,\n                                           score_threshold=0.05,\n                                           iou_threshold=0.5,\n                                           max_output_size=200)\nfinal_class_ids = tf.gather(class_ids, nms_indices)\nfinal_scores = tf.gather(confidence_scores, nms_indices)\nfinal_boxes = tf.cast(tf.gather(boxes, nms_indices), dtype=tf.int32)\n\nmatched_anchors = tf.gather(anchors, tf.where(confidence_scores > 0.05)[:, 0])\nmatched_anchors = tf.cast(change_box_format(matched_anchors, return_format='x1y1x2y2'),\n                      dtype=tf.int32)\nimg = draw_boxes_cv2(image, boxes)\nimshow(img)\nprint(final_boxes.numpy())\nprint(final_class_ids.numpy())\nprint(final_scores.numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RetinaNet(input_shape=INPUT_SHAPE, n_classes=N_CLASSES)\nloss_fn = LossV2(batch_size=BATCH_SIZE, n_classes=N_CLASSES)\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=1e-3)\n[x.shape for x in model(tf.random.normal((1, INPUT_SHAPE, INPUT_SHAPE, 3)))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef training_step(batch):\n  image, (classification_targets,\n          regression_targets,\n          background_mask,\n          ignore_mask) = batch\n  with tf.GradientTape() as tape:\n      classification_predictions, regression_predictions = model(\n          image, training=True)\n      Lreg, Lcls, num_positive_detections = loss_fn(classification_targets,\n                                                    classification_predictions,\n                                                    regression_targets,\n                                                    regression_predictions,\n                                                    background_mask,\n                                                    ignore_mask)\n      total_loss = Lreg + Lcls\n  gradients = tape.gradient(total_loss, model.trainable_variables)\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n  return Lreg, Lcls, total_loss, num_positive_detections\n\n\n@tf.function\ndef validation_step(batch):\n  image, (classification_targets,\n          regression_targets,\n          background_mask,\n          ignore_mask) = batch\n  classification_predictions, regression_predictions = model(\n      image, training=False)\n  Lreg, Lcls, num_positive_detections = loss_fn(classification_targets,\n                                                classification_predictions,\n                                                regression_targets,\n                                                regression_predictions,\n                                                background_mask,\n                                                ignore_mask)\n  total_loss = Lreg + Lcls\n  return Lreg, Lcls, total_loss, num_positive_detections","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for ep in range(0, 1):\n    for step, batch in enumerate(train_dataset):\n        Lreg, Lcls, total_loss, num_positive_detections = training_step(batch)\n        logs = {\n            'epoch': '{}/{}'.format(ep + 1, EPOCHS),\n            'train_step': '{}/{}'.format(step + 1, training_steps),\n            'box_loss': np.round(Lreg.numpy(), 2),\n            'cls_loss': np.round(Lcls.numpy(), 2),\n            'total_loss': np.round(total_loss.numpy(), 2),\n            'matches': np.int32(num_positive_detections.numpy())\n        }\n        if (step + 1) % 10 == 0:\n            print(logs)\n    for step, batch in enumerate(val_dataset):\n        Lreg, Lcls, total_loss, num_positive_detections = validation_step(\n            batch)\n        logs = {\n            'epoch': '{}/{}'.format(ep + 1, EPOCHS),\n            'val_step': '{}/{}'.format(step + 1, validation_steps),\n            'box_loss': np.round(Lreg.numpy(), 2),\n            'cls_loss': np.round(Lcls.numpy(), 2),\n            'total_loss': np.round(total_loss.numpy(), 2),\n            'matches': np.int32(num_positive_detections.numpy())\n        }\n        if (step + 1) % 25 == 0:\n            print(logs)\n    model.save_weights('{}_epoch_weights.h5'.format(ep+1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}